\documentclass[a4paper,notoc,oneside]{tufte-book}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{lmodern}
\author{Micky Faas}
\title{Spatial Compression-Based Pattern Mining with VOUW}

% We'll abuse a book class to function as a report: get rid of those extra blank pages!
\makeatletter\@openrightfalse\makeatother 

\begin{document}
\maketitle

\chapter*{Abstract}
\begin{itemize}
\item MDL is a principle that utilizes compression as a means of describing data
\item There exist many MDL-based algorithms for a variety of problems
\item Few (no?) solutions exist to mine grid-like data based on MDL
\item VOUW is a novel approach to discover patterns and structural relations in 2D, discrete datasets
\item We propose both a theoretical framework as well as a complete, optimized implementation 
\end{itemize}

\setcounter{tocdepth}{3}
\tableofcontents

\chapter{Introduction}

Introduce the topics that will be discussed in this thesis.
\begin{itemize}
\item In this chapter we will introduce foundations such as Kolmogorov-complexity, MDL, entropy, etc.
\item Furthermore we will give a brief description of the problem, with some examples
\item The introduction is concluded with a section on related work
\item In the chapter `Theoretical Framework' we discuss the formal part of VOUW, with all of its definitions and a more complete description of the problem
\item In the chapter `A Search Algorithm' we will talk about the actual search algorithm and extensions thereof 
\item The (short) chapter `Practical Implementation' will be about actual code and QVouw, the GUI
\item `Experiments' will try to empirically show some of the concepts introduced in the previous two chapters
\item And finally `Discussion' and `Conclusion' will conclude the thesis
\end{itemize}

\section{General Problem Description}

VOUW is a framework/set of definitions, a practical algorithm an academic implementation of these concepts. We use explanatory data mining to discover the structure of a dataset. To goals is not only to describe the data as concise as possible, but also how we derive the vocabulary to make that description. This concept can be extended from analysis to similarity and clustering: want we not only be able to tell how similar two datasets are, but also why they are (dis)similar. 

\subsection{Brief Example}

I would like to illustrate the problem with a small example, such that the reader's interest is (hopefully) piqued. This example could be a small fabricated matrix or (ideally) a toy example that bears some real-world relevance.

\subsection{Grids, Images and Matrices}

The problem is defined on a specific type of input:
\begin{itemize}
\item Matrix-like data, but not in the linear algebra sense of the word. The closest description would be an image, because it also is a rigid, grid-like dataset where rows and columns have a fixed ordering.
\item VOUW is, however, not an image mining algorithm and therefore we use the term `matrix'.
\item We use 2D input only. The algorithm and definitions could be expanded for multi-dimensional matrices. 
\item Individual data points must be discrete.
\item Matrix can be sparse, but low-density matrices yield no useful patterns and thus the algorithm has limited usability on sparse matrices.
\end{itemize}

\subsection{Patterns}

In order to gain better understanding of the input data, we try to express it in terms of a set of \emph{patterns}. In this context a pattern is:
\begin{itemize}
\item A submatrix of the original matrix
\item A structure that occurs more than once, either exact or with some degree of `noise tolerance'
\end{itemize} 
A complete, loss-less description does not only contain patterns, but also tells us where they are located in the original matrix. We call the placement of a pattern on the original matrix an \emph{instance} of a pattern. A description of the original matrix is valid only if each element belongs to exactly one instance. There are, however, many valid descriptions possible. The third chapter discusses a search algorithm that tries to approach the optimal.

\subsection{The Problem and Its Classes}

Apart from a formal definition, it is useful to semantically define what kinds of problems we would like to be able to solve. Therefore a division in five different classes was made.
\sidenote{
\begin{description}
\item[Class 1a] Exact duplicates in a sparse matrix.
\item[Class 1b] Exact duplicates in (differently distributed) noise.
\item[Class 1c] Exact duplicates in noise with equal distribution.
\item[Class 2 ] Approximate duplicates
\item[Class 3 ] Transformed duplicates
\end{description}
}

\chapter{Related Work}

The most tedious but much-needed section.

\chapter{Preliminaries}

\section{Data Mining Through Compression}

In the previous section we have discussed that the input matrix can be expressed in terms of patterns and instances, and that some combinations are more favourable than others. We need a way to quantize the quality of a description and we do this according to the MDL-principle. 

The Minimum Description Length Principle:
\begin{itemize}
\item Is a practical realization of Kolmogorov-complexity, which in itself is shown to be incomputable
\item Has multiple variants; we use the `crude' or two-part MDL equation
\item Is an optimization problem that tries to minimize the total length of the description of the input data
\item It can be shown that the most concise (but loss-less) description of the original data reveals the most information about the data
\item Finding a minimal loss-less description can be thought of as \emph{compression}, hence `datamining by compression'
\item This minimization problem usually has a huge search space and therefore most approaches use a greedy heuristic
\end{itemize}

Two-part MDL splits the description into the \emph{model} and the \emph{data given the model}. Minimizing the sum of their lengths prevents over- and underfitting. \emph{(Explain these terms in the context of our problem)}

\subsection{Priors and Encoding}

Apart from finding or approaching the optimal minimal description, the greatest challenge in MDL-based algorithms lies in the fact that we have to devise some kind of encoding scheme to compress the data. We must make this encoding scheme in such way that it can be decoded without loss of information.
\begin{itemize}
\item We will not actually encode the data each time, but rather just compute the length if it were encoded
\item There are multiple ways (\emph{priors}) of assigning code lengths to symbols according to their probability of occurring. \begin{itemize}
\item If we know the total number of symbols and the distribution is uniform, we use $-\log(p)$. This is the optimal code length accordig to Shannon's entropy. 
\item For integers on an unknown domain there is the Universal Prior for integers. 
\item Lastly, we use prequential-plugin code if we do not know the distribution in advance, but want to approach the uniform distribution for large sets of symbols.
\end{itemize}
\end{itemize}

\chapter{Theoretical Framework}

\emph{This is the part that is largely done. I will only describe sections that are new.}

\section{On Patterns and Matrices}
\subsection{Pattern and Instances}
\subsection{Configurations (previously: Structural Equivalence)}

\section{The Problem and its Solution Space}
\subsection{Constructing Patterns (moved)}
\subsection{The Optimal Model}

\section{Encoding Models and Instantiations}

\emph{Decided to move this into a separate section}. 

\chapter{A Search Algorithm}

\emph{I have this partially, but so much has changed that it will need to be rewritten anyway. I would like to split it in two parts: the absolute minimal baseline algorithm in one section, and then another to describe all the extensions and optimizations.}\\

\section{An Outline}

Given an input matrix, the algorithm is initialized with: (a) probability mass function of the values in the matrix, (b) singleton pattern for each unique element in the matrix and (c) the under-fit instantiation matrix containing only singletons.  The baseline algorithm is composed out of four stages that are repeated every iteration:
\begin{enumerate}
\item Candidate search
\item Gain computation
\item Candidate selection
\item Merging of patterns and their instances
\end{enumerate}

\subsection{Candidate Search}
\subsection{Gain Computation}
\subsection{Selecting and Merging Candidates}

\section{Extensions and Improvements}

\subsection{Pre-processing}
\subsection{Reducing Self-Overlap}
\subsection{Alternative Heuristics}
\subsection{Local Search}
\subsection{Noise-Tolerance}

\chapter{Practical Implementation}

\emph{This is a chapter for the thesis only to explain a selection of (possibly) interesting decisions made during the implementation. I want to keep this brief}
\begin{itemize}
\item Datastructure for the candidate map and how it is generated
\item Datastructures for the InstanceVector and InstanceMatrix, and why this construction improves time-complexity
\item Brief introduction of QVouw and what it offers
\item Availability
\end{itemize}

\chapter{Experiments and Results}

\emph{Outline of possible experiments:}
\begin{itemize}
\item Quality of the pattern set.
\begin{itemize}
\item Optimal vs heuristics (not practical, scaling/generalization issues)
\item Synthetic (pre-generated models generate data, fun, let's program that)
\item Compression ratio (apparently there is a theoretical basis on why these metrics are representative)
\end{itemize}
\item Baseline vs optimizations (will optimizations alter quality of the outcome?)
\item Anecdotal evidence (examples, demonstration)
\end{itemize}

\chapter{Discussion}

\chapter{Conclusion}

\end{document}