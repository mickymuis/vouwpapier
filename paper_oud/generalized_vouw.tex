\documentclass{llncs}
\usepackage{llncsdoc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{color}
\usepackage{gnuplottex}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{microtype}
\captionsetup{compatibility=false}

%\algloopdefx[IfContinue]{IfContinue}
%[1]{\textbf{if} #1 \textbf{then break}}

\algsetblockdefx[IfContinue]{IfContinue}{IfContinue}
{0}{0pt}
[0]{}
[1]{\textbf{if} #1 \textbf{then continue}}

\begin{document}
\section{VOUW: A Framework}

The very first objective of this paper is to generalize our ideas into a theoretic framework. Such a framework provides us with the necessary tools to formally define the problem and the idealized space of solutions. The VOUW framework consists of two parts: (1) an concise definition that allows us to reason about what one can and cannot expect to find from a set of data and what this data could look like, and (2) a generalized encoding scheme that connects our framework to the MDL-principle. \\
\textit{De volgende twee alinea's moeten nog wat preciezer, ofwel negeer ze maar gewoon.}
\\

As previously introduced, the overall objective is to create a description $D'$ of some dataset $D$ that is both optimal in length and in the amount of semantically relevant information it provides on $D$. To this end we seek to derive a model $H$ and a set of residuals $D|H$ (literally `the data given the model') such that $D'=\{H,D|H\}$. Given that $D'$ is optimal, there is exactly one $D'$ for a given $D$ and this $D'$ is unique ($D \mapsto D'$ is bijective). Readers might already be familiar with this relation from the MDL-principle, and indeed, we will later use this principle to show that the \emph{smallest} $D'$ is the most optimal.

From now on, let us take $D$ to be some matrix $A$. Its optimal model $H_A$ should be able to capture all recurring structure, or patterns, in $A$ that is semantically relevant, while containing nothing else. Informally we could say that $H_A$ is the `perfect summary' of $A$. In addition to $H_A$, there is the set of residuals $A|H_A$. These residuals can be thought of what is normally called the `error' or `noise' in $A_H$. Another, more meaningful, way to look at $A|H_A$ is to think of it as a mapping from $H_A$ back to $A$. Indeed we will see that given $A|H_A$ we can unambiguously construct $A$ from $H_A$. We will therefore call $A|H_A$ the set of (pattern) \emph{instantiations} from this point on. In this light it is not hard to see that we are actually losslessly \emph{compressing} $A$ by finding a set of symbols (the model) $H_A$ and encoding $A$ using these symbols in $A|H_A$ (the instances).%, just as follows from the Kolmogorov-complexity theory.

\subsection{Problem Domain}

The VOUW framework is defined on grid-like data that is both bounded and discrete. This paper takes $A$ to be a rectangular $M\times N$ matrix, but both the shape and dimensionality of $A$ could in theory be different. $A$ is defined as a typical matrix with elements $a_{i,j}$ where row $i$ is on $[0;N)$ and column $j$ is on $[0;M)$. Furthermore we say that $a_{i,j} \in \mathbb{N}$, e.g. is of the integral type, and that $0 \leq a_{i,j} < b$. $b$ of $A$, written as $b(A)$, is said to be the \emph{base} of $A$ and this number basically tells us the number of different values $A$ contains. $b=2$ is a special case and makes $A$ a \emph{boolean matrix}. While it is strictly not imperative that the base is known beforehand, both the encoding and some data-specific operators can be greatly simplified if it is.

Now that we know what the domain of our data looks like, it is time to state what it is that we are looking for. Since we are mining for patterns in $A$ it makes sense to informally say that a model $H_A$ should be a set of `patterns' over $A$. This set should only contain unique patterns and not more than absolutely necessary, otherwise it would not be optimal. Conversely the set of pattern instantiations must contain `everything that is not a pattern'. But what exactly is a pattern? We will try to formalize these concepts in the next sections.

%Now that we know what $A$ looks like, it is time to define what makes an optimal model $H_A$. In an ideal case we would not `look' for any specific structure in $A$, we would rather try to define $A$ in the simplest, most optimal way possible. In order to make this search more specific however, we also assume some prior knowledge about $A$. We will introduce a \emph{domain-specific equivalence} that enables us to discard semantically irrelevant candidates early and thus eliminate some of the complexity. Using this equivalence, we can formulate the requirement that $H_A$ is the set of all \emph{unique} patterns in $A$. Our goal is to find this set by disimageing spatial patterns in $A$ recursively by combining previously found patterns into increasingly large patterns. Now that we have this top-down idea of a model we can define what we actually mean by a `pattern' and we will do so in the next section.

\subsection{Patterns}
We can encode a collection of elements $a_{i,j}\in A$ with a \emph{spatial offset} $(i,j)$ and a \emph{magnitude offset}. A set of these offsets represents a \emph{pattern} over $A$. 
\begin{definition}A \textbf{pattern} is a set of pairs in the form $(\vec{v},w)$ , where $\vec{v}$ is an n-dimensional spatial offset within $A$ and scalar $w$ is a magnitude offset. $\vec{v}$ is unique, i.e. it occurs at most once in a pattern.
\end{definition} 
\begin{definition}\label{leq}
The relation $\leq$ on pairs $x, y\in X$ is defined for a pattern X such that $x\leq y$ iff for $\vec{v}_x=(i_x,j_x)$ and $\vec{v}_y=(i_y,j_y)$ holds that $2M+(N+i_x-1)(2M-1)+(M+j_x-1) \leq (N+i_y-1)(2M-1)+(M+j_y-1)$.
\end{definition}
\begin{theorem} Any pattern X is a totally ordered set.
\end{theorem}
\begin{proof}
We use the relation $\leq$ that we have defined on elements of $X$. Notice that $X$ is a set of pairs but only the first element $\vec{v}$ is considered by the relation. By this definition we can already say that $X$ is partially ordered: because $\vec{v}=(i,j)$ and $j$ is bounded by $M$ we can map $(i,j$) onto $\mathbb{N}$ by $n = (N+i-1)(2M-1)+(M+j-1)$. This gives us reflexivity, antisymmetry and transitivity as on $\mathbb{N}$ by the less-than-equal relation. We use the fact that $\vec{v}$ is unique in any given $X$ to show that for $x_i,x_j\in X$ either $x_i\leq x_j$ or $x_j\leq x_i$ and thus satisfies comparability, making X totally ordered.  
\end{proof}
\begin{definition}Pattern $X$ is a \textbf{singleton pattern} if $|X|=1$
\end{definition}
The scalar $w$ in each pair $(\vec{v},w)$ is the magnitude offset. Let us define what we exactly mean by that. Imagine we create a singleton pattern $X=([0,0],2)$ of elements in $A$. $X$ contains one element that has magnitude $2$. So simply said, it matches all elements $a_{i,j}=2$. In this case the magnitude offset is taken from zero, the lowest possible value in $A$. Now let us define a pattern $Y$:
$$Y=\big\{X_0+([0,0],0), X_1+([1,0],1),X_2\dots\big\}$$
In this notation we define the $+$ operator as $X+\omega=\{x + \omega | x \in X\}$. Let us take $X_{0,1}$ to be $X$. Clearly, the pattern $Y$ contains all elements of $X$ at their origin and offset by $([1,0],1])$. The magnitude offset is in this case relative to $X$ and yields $Y=\{([0,0],2),([1,0],3)\}$, matching all horizontal adjacent elements $2, 3$ in $A$. Now assume that such a set of elements actually occur in $A$ starting from $\vec{p}=(x,y)$. These elements are then said to be the \emph{image} of $Y$ at $\vec{p}$. We call such a vector $\vec{p}$ a \emph{pivot}. A pivot is just another offset that shifts the center $(0,0)$ of the pattern by some $(x,y)$.
\begin{definition}
The \textbf{image} in $A$ of a pattern $X$ at pivot $\vec{p}$ is the set of elements $a_{i,j}\in A$ such that $(i,j)=\vec{v}+\vec{p}$ for each $(\vec{v},w) \in X$. 
\end{definition}

Recall that $H_A$ is a set of patterns over $A$, but which patterns are there exactly? For this we define $\tilde{\mathcal{P}}_A$ to be the set of all patterns over $A$. It contains the singleton patterns of $A$ all the way up to the most complex pattern (i.e., $A\in \tilde{\mathcal{P}}_A$). While this is certainly useful, consider the following definition. 
\begin{definition}
Pattern $X$ is \textbf{isomorphic} to pattern $Y$ if there exists some pivots $\vec{p},\vec{q}$ for which the image of $X$ at $\vec{p}$ equals the image of $Y$ at $\vec{q}$. We write $X\cong Y$.
\end{definition}

Another important observation is that for a pattern $X$ to occur in $A$, its values not need be strictly equal to the values of the corresponding elements $a_{i,j}$, as long as it has the same meaning in the context of the data. We call this \emph{semantic equivalence}.
\begin{definition}
Patterns $X$ and $Y$ are \textbf{semantically equivalent} in $A$ if there exists some $t$ for which ${X\circ t}\cong Y$. We write $X\equiv Y$. 
\end{definition}
\noindent We call $X\circ t$ a \emph{variant} and $\circ$ some relation on a set of elements of $A$ that transforms $X$ to $Y$ using $t$. We restrict $\circ$ in such way that it cannot add or remove elements from $X$ and that any variant $X\circ t$ is unique.

Let us take for example the strings $(1,0,0,1)$ and $(0,1,1,0)$ from some dataset. While having a completely different absolute value, one could say that there are equivalent if we define $X\circ t$ as $\forall_{x\in X} \cdot {x+t\!\mod B}$. We can easily see that $(1,0,0,1)\circ 1 =(0,1,1,0)\circ 0$. The variants here are $1$ and $0$ respectively. Another example is the equivalence of the vectors $\begin{pmatrix}
1 \\ 1\end{pmatrix}$ and $(1,1)$. In this case the relation $\circ$ would be equal to $t$ times their transpose. Many different relations and combinations thereof are possible, as long a they are semantically meaningful for the data in $A$. The elegance in this is that we have abstracted this domain-specific knowledge away into one single operator.

It would seem that we have now two different equivalence relations defined on patterns: isomorphism and (semantic) equivalence. The set $\tilde{\mathcal{P}}_A$ contains both isomorphic patterns as well as semantically equivalent patterns. It is not hard to see that this would lead to unnecessary complexity. For all isomorphic and equivalent patterns we would like to pick one unambiguous representative.
\begin{definition}
Pattern $X$ is in \textbf{canonical form} if for all elements $x_i=(\vec{v}_i, w_i)$ hold that $v_0=(0,0)$ and $v_i\leq v_{i+1}$.
\end{definition}
\begin{theorem}
If $X$ and $Y$ are in canonical form, $X\cong Y$ if and only if $X=Y$.
\end{theorem}
\begin{theorem}\label{thmexistcano}
For any pattern $X$ not in canonical form, there exists a pattern $X'$ in canonical form.
\end{theorem}

For equivalence it is not possible to pick a single representative because the ordering of elements in a pattern is only defined using spatial offset and not by magnitude. Furthermore, the relation $\circ$ is a black box to us. We know, however, that any given pattern has a finite number of variants because $\circ$ cannot add or remove elements and the set of values of $a_{i,j}$ is finite. Therefore we can pick as representative a finite set of equivalent patterns.
\begin{definition}
The \textbf{equivalence set} $X^*$ of canonical pattern $X_0$ is the set of all canonical patterns $X_0,X_i,\dots$ such that holds $X_i \equiv X_{i+1}$.
\end{definition}
\begin{theorem}
If some pattern $X_i \in X^*$ then $X_i*=X^*$, i.e. the set $X^*$ is complete and unique.
\end{theorem}

This is still not good enough as we want to be able to enumerate the elements of $X^*$. We therefore define another total ordering on $X^*$ as follows: We know that every pattern in $X_i\in X^*$ is in canonical form and thus the order of the elements in these patterns is fixed. $\circ$ tells us that the number of elements is also finite, say it is $n$. We now can represent each $X_i\in X^*$ as a number $k=k_0k_1\dots k_n$ of $n$ digits in base $b$, one for each element $x_{ij}\in (X_i)$ such that $k_j=\omega_j$. However if elements are just repositioned by $\circ$ this number $k$ may not be unique. We therefore order all patterns with equal $k$ by mapping them onto another $n$-digit number $l_0,\dots,l_n$ but this time using $\l_j=\vec{v}_j$. To order the vectors $\vec{v}$ we define $\leq$ as in Definition \ref{leq}. Lastly we define $(X^*)_a\leq (X^*)_b$ as $k_a \leq k_b \lor l_a \leq l_b$. This gives us a partial ordering while the requirement on variants to be unique fulfils comparability and gives us a total ordering on $X^*$.

The ordering of $X^*$ conveniently lets us enumerate all patterns equivalent to $X$. 
\begin{definition}
$X^*_0$ is the pattern in $X^*$ such that no pattern $(X^*)_i \leq X^*_0$ exists. We define $X^*_t$ as the $t$-th pattern in $X^*$ such that $X^*_t=(X^*_0)\circ t$.
\end{definition}

Using $X^*_0$ we define another, more useful set of patterns over $A$. We say that the set $\mathcal{P}_A$ is the set of all patterns $X^*_0$ over $A$. A special set $\mathcal{P}_A^0$ is the set of all $X^*_0$ with length 1 (thus sets of singleton patterns).

\subsection{Regions}

With the definitions given above we can almost formalize the definition of model $H_A$ and its instantiations. Because we want a model to only contain unique patterns, we say that any $H_A \subseteq \mathcal{P}_A$. As mentioned before, patterns can occur any number of times in $A$ at different pivots. We call a tuple of a pivot, a pattern and variant a \emph{region}. 
\begin{definition}
A \textbf{region} is a tuple in the form $(X,t,\vec{p})$ such that $X\in \mathcal{P}_A$ and the image $X^*_t$ occurs at $\vec{p}$ in $A$. The image of a region $(X,t,\vec{p})$ is defined as equal to the image of pattern $X^*_t$ at pivot $\vec{p}$.
\end{definition}
Indeed a region is just the smallest amount of information required to map a pattern onto its image in $A$. Unsurprisingly the set of instantiations to $H_A$ can be defined as a set of regions:
\begin{definition}
The instantiations $\{A|H_A\}$ of $H_A$ is a set of regions $\{R_1,R_i,\dots,R_N\}$ such that the union of the images $\displaystyle\bigcup_{i=1}^{N} \mathtt{image}(R_i)$ equals $A$.
\end{definition}

\subsection{Constructing Patterns and Regions}

As patterns are the building blocks of our model we would like to be able to have an intuitive way to construct increasingly large patterns. One way to do this is to define some join operation on two patterns such that the result is a new pattern containing elements from both. A simple union would not work in this case, as this would disregard any spatial relation those two patterns might have. We therefore have to define a special `pattern union' that takes a third argument, namely the distance between the two operands. 

\begin{definition}
The union of patterns $X$ and $Y$ with distance $\vec{\delta}$ is defined as $X\cup\{y_i + (\vec{\delta},0) \ | \ y_i \in Y\}$ for any $X,Y$ such that $X\cap Y=\emptyset$.
We write $X\bigcup^{\delta}Y$.
\end{definition}
We can immediately see that $\bigcup^{\delta}$ is a more restricted version of $\bigcup$. Because we take $\vec{\delta}$ to be the distance from $X$ to $Y$ where $X$ is the origin, properties such associativity and commutativity do not hold for $\bigcup^{\delta}$. Another concern is that if $X$ and $Y$ are in canonical form, is $X\bigcup^{\delta}Y$ too? The answer is, fortunately, yes (though we will not show it here). 

The union of patterns has its counterpart for the set of instantiations. We can define a special sum on two regions (recall a region is a tuple not a set) by internally using the pattern union we just defined.

\begin{definition}
The sum of regions $R=(X,t,\vec{p})$ and $S=(Y,u,\vec{q})$ is defined as $R+S=(Z*0,t',\vec{p})$ where $Z=(X\circ t) \bigcup^{\delta} (Y\circ u)$, $\vec{\delta}=\vec{q}-\vec{p}$ and $(Z*0)\circ t'=Z$.
\end{definition}

This is in fact a quite complex definition and it is also the fundamental operation of VOUW. Again we take the first operand as the origin such that $R+S\neq S+R$. Before the pattern union is computed between the patterns of $R$ and $S$, notice how the variants are calculated first. The resulting pattern will have a different structure/number of elements and will thus belongs to a different equivalent set - which is unknown. This is circumvented by taking $Z*0$ rather than $Z$ to be the pattern of $R+S$. We use both this and the $*$ operator to find that $Z$ is the $t$-th element in $Z*$. Finally we take this $t'$ to be the variant of $R+S$.

Notice that the region sum has the same limitation as the pattern union: two regions can only be summed if they do not overlap, or in other words, their patterns need to be disjunct (after applying variants and $\vec{\delta}$). While this is a serious limitation, we will show in the next section that it is not of any practical relevance.

\subsection{The Set $\mathcal{H}_A$ and $\bar{\mathcal{H}}_A$}\label{thesetH}

Now all the pieces are ready to define the set of all possible models of $A$ and the set of all possible instantiations to models of $A$. We will write these sets as $\mathcal{H}_A$ and $\bar{\mathcal{H}}_A$ respectively. To this end will first take $H_A^0$ to be the model with only singleton patterns (patterns of length 1). Note that as all unique singleton patterns are required to create a singleton model of $A$ and therefore $H_A^0=\mathcal{P}_A^0$. The instantiations to $H_A^0$ are the set $A|H_A^0$. This is the set of regions that map one singleton pattern on every $a_{ij}$. Because every $a_{ij}$ is only the image of exactly one variant of a patterns in $H_A^0$, there is only one way to do this.
\begin{theorem}
Given a matrix $A$ and the set of unique patterns $\mathcal{P}_A$, $H_A^0$ and $A|H_A^0$ are also unique.
\end{theorem}
Now we inductively define the set $\bar{\mathcal{H}}_A$ of all instantiations of all models over $A$:\\
\textbf{Base case:} $A|H_A^0 \in \bar{\mathcal{H}}_A$\\
\textbf{By induction:} If $\bar{h}$ is in $\bar{\mathcal{H}}_A$ then for any $R,S \in \bar{h}$, the set $h\setminus\{R,S\}\cup \{R + S\}$ is also in $\bar{\mathcal{H}}_A$.

Notice how the addition of two regions is the fundamental operation here. Indeed we can add any two regions together in any order and eventually this results in just one big region: $A$. The elegance in this is that by this inductive definition, the regions $R$ and $S$ never overlap and thus their sum is always valid. By the same principle this sum is always a pattern with an image in $A$.

The construction of instance sets also implicitly defines the corresponding models. While this may seem odd - defining models for instantiations instead of the other way around - note that there is no unambiguous way to find a instance set for a given model. Instead we find the following trivial definition by applying the inductive construction rule above.
\begin{definition}
The set $\mathcal{H}_A$ of all models over $A$ is the set such that $\mathcal{H}_A=\Big\{\{X \ | \ (X,t,\vec{p})\in h\} \ \Big | \ h \in \bar{\mathcal{H}}_A \Big\}$. 
\end{definition}
So given any instance set $\bar{h}\in \bar{\mathcal{H}}_A$ there is a corresponding set in $\mathcal{H}_A$ of all patterns that occur in $\bar{h}$. This finalizes the definition of models and instantiations.
\begin{theorem}
Given any instance set $\bar{h}\in \bar{\mathcal{H}}_A$ and its corresponding model in $\mathcal{H}_A$, the matrix $A$ can be reconstructed unambiguously.
\end{theorem}

\subsection{The Optimal Model}

We have now found \emph{all} possible models for $A$ and have defined a nice method to construct both instance sets and models inductively. While this is certainly useful, we still need to find \emph{the optimal} model. To this end we make the connection with the MDL-principle that says, informally, by minimizing the following equation the given model is optimal.
$$
L_1(H_A) + L_2(A|H_A)
$$
Here the functions $L_1, L_2$ are two independent length functions that make up the coding scheme for two-part MDL. Recall that we essentially want to compress the data in $A$. We therefore need to find an encoding scheme that encodes both model and instantiations lossless and without redundancy.

Say we have some set of symbols $S=\{S_0,S_1,\dots,S_n\}$ and each symbol has a length $l_0,l_1,\dots,l_n$. These symbols could be for example a code word for each pattern in a model. We now want to find lengths $l_i$ using the Kraft-inequality such that holds that $\displaystyle\sum^N_{i=0}r^{-l_i} \leq 1$. In this case we say that $r=2$, because there are two symbols (0 and 1) in our code alphabet so we can measure code lengths in bits. 
The Kraft-inequality gives us a bijection between code lengths and probability distributions. This is one of the main ideas of the MDL-principle. So in our example we can write a probability distribution $P(X), X \in H_A$ as the chance of $X$ occurring in our instance set. We can then use Shannon's Entropy $l_i=-\log(P(X_i))$ to compute the exact number of bits a pattern should optimally be encoded with.

Notice how common code words are shorter then rarely used code words. According to the Kraft-inequality this gives us an optimal code. The number of bits we compute are however, real numbers and not integers. While this does certainly not result in a practical encoding, we do not actually need to encode the data as we are only interested in its hypothetical length.

\subsection{Encoding Models and Instantiations (niet lezen)}
Even though we will not actually be encoding $A$, we want the calculation of the encoded length to be as precise as possible. The simple reason for this is that the only information that we provide to the MDL equation and thus the optimization process, is this encoded length. A coarse granularity may lead to unwanted results and overfitting. While this may sound inexact we will  make it more precise by using Shannon's Entropy to reason about the optimal length for any element we encode.

From the definitions of $\mathcal{H}_A$ and $\bar{\mathcal{H}}_A$ we know that patterns both occur as elements in $H_A$ as well as in the regions of $A|H_A$. It makes sense that we encode the complete patterns once for the models and then refer to them from each region by a code word. This makes the practical materialisation of the model a \emph{code table} that maps each pattern to some code word. So how do we generate these code words? Recall that we are only interested in the theoretical length of a code and not so much in the actual word itself. To compute the optimal length we first and foremost need to find out what the chance is that a certain pattern occurs and this in turn depends on the prevalence in the dataset.
\begin{definition}\label{usage}
Given a set of instantiations $A|H_A$, we take $\mathtt{usage}(X)$ of a pattern $X$ to be the prevalence of $X^*$ in $A|H_A$. More precisely 
$$\mathtt{usage}(X) = |\{R\mid R=(Y,t,\vec{p}) \in \{A|H_A\}, Y^*=X^*\}|$$
\end{definition}
From this definition we see that the \emph{usage} of a pattern is sum a of how often any of its variants occur as an instance. Using this function we find the probability that a certain pattern occurs simply by $P(X)=\frac{|\{A|H_A\}|}{\mathtt{usage}(X)}$. The optimal length of a code word $L^C$ can then be found by Shannon's Entropy. 
Now that we know the length of a code word the pattern itself must also be encoded. Recall that a pattern consists of pairs of spatial and magnitude offsets. In a $M\times N$ matrix there can be at most $4(M-1)(N-1)$ distinctive spatial offsets and this number can be used to obtain the minimum number of bits each offset can be encoded with. The magnitude offset depends on the number of possible values in $A$, which is $b(A)$. Assuming the distribution of values in $A$ is uniform, we can also use a fixed value here. This brings the length of an encoded pattern to:
$$
L(X)= |X| \cdot \bigg(-\log\Big(\big(4(M-1)(N-1)\big)^{-1}\Big) -\log\Big(b(A)^{-1}\Big)\bigg)
$$
This leaves only the instantiations still to be encoded, which form a list rather than a table. Each region simply refers to its pattern X by using the code word we computed earlier and we already know its length. The pivot is again a fixed number that depends on the total number of possible pivots - $MN$ in this case. Encoding the variant is harder because we have never clearly defined $|X^*|$. In principle the upper bound for the number of variants for a given pattern $X$ is $b(A)^{|X|}$, since we established $X^*$ is at least finite. This is not a practical figure however, given that there probably are far less elements in $A$. A solution is to limit the total number of variants for $X$ to the number that \emph{we know about} at a given moment. We can find this number in a similar way to the \texttt{usage} function. 
\begin{definition}
Given a set of instantiations $A|H_A$, we define
$$\mathtt{variants}(X) = |\{Y\circ t\mid R=(Y,t,\vec{p}) \in \{A|H_A\}, Y^*=X^*\}|$$
\end{definition}
Which basically defines $\mathtt{variants}(X)$ as the number of distinct variants of $X$ that occur within $A|H_A$. In a similar way as the encoded length of a pattern, we can now define the encoded length of region.
$$
L(R) = -log\big(MN^{-1}\big) -\log\big(\mathtt{variants}(X)^{-1}\big) + L^C(X)
$$
Where $X$ is the pattern in $R$.
We can now compute the lengths of both region and patterns as well as the length of a code word for each pattern. By summing of all patterns and regions we can finally give the total length of the model and the set of instantiations.
\begin{align*}
L(H_A) &= \displaystyle\sum_{X\in H_A} L^C(X) + L(X) \\
L(A|H_A) &= \displaystyle\sum_{R\in A|H_A} L(R)
\end{align*}


\section{VOUW: An Algorithm}

Now that we have finalized the theoretical foundation we can begin to formulate the problem and work towards an algorithmic solution. Before we do so, however, we take a moment to sketch the complexity of the current solution space. In order to do this we use the same steps as the inductive definition of $\bar{\mathcal{H}}_A$ as given in section \ref{thesetH}. We start with $\bar{h}=\bar{H}_A^0$, which by definition contains exactly $MN$ regions. We then pick two arbitrary regions and replace these with their sum to obtain $\bar{h}'$. Simple combinatorics say we can do this in $\binom{MN}{2}$ ways. Since now $|\bar{h}'| = MN-1$, we have $\binom{MN-1}{2}$ possibilities for the next step, and so forth until we have only one big region left that contains exactly $A$ (the completely overfit model). We can simplify this sequence to
\begin{align}
\displaystyle\prod^{MN-2}_{n=0} \binom{MN-n}{2} = \displaystyle\prod^{MN-2}_{n=0} \frac{MN-n}{2(MN-n-2)!} = \frac{(MN)!(MN-1)!}{2^{MN-1}} .
\end{align}
While we must stress that this is an upper bound in the worst case that no pattern ever occurs more than once, the complexity class this naive approach yields is still quite impossible. In the remainder of the section we will describe a baseline algorithm that utilizes heuristics and additional constraints to reduce this complexity to close to linear.

\subsection{Heuristics}

It appears that it is not feasible to search the entire solution space due to the combinatorial explosion we have just seen. We will now describe a practical baseline algorithm that can search the solution space in close to linear time, if we are willing to find `a good solution' rather than `the optimal solution'. The baseline algorithm can be used to show complexity and give guarantees on convergence, while we will later modify it for practical applications by adding decomposition (a form of pruning) and support for domain-specific equivalence.

The first step is to add two additional constraints to the search:
\begin{enumerate}
\item The search must be \textbf{greedy}: when we unite two regions, we will never go back to a state where they are separate.
\item Two regions can only be united if they are \textbf{adjacent}.
\end{enumerate}

The first constraint is very straightforward. The greedy approach reduces the complexity considerably with the danger of converging on local minima. While this sounds problematic, notice that the discussed models are already an approximation. Therefore it is hard to justify the computational expenses looking for `the optimal approximation'. Furthermore, we will introduce an additional method later on that decreases the probability of convergence on local minima.

For the second constraint we must first define what we mean by \emph{adjacency}.
\begin{definition}
Two regions $R=(X,t,\vec{p})$ and $S=(Y,u,\vec{q})$ with $(i,j)=\vec{q}-\vec{p}$ are \textbf{adjacent} if
$$
	\vec{p} < \vec{q} \land j \leq \mathtt{colMax}(X)+1 \land j \geq \mathtt{colMin}(X)-1 \land i \leq \mathtt{rowMax}(X)+1.
$$
\end{definition}

For $\vec{p} < \vec{q}$  we use the same ordering for two-dimensional vectors as in Definition \ref{leq}. This ensures that the position of $R$ is always before $S$. This solves problem the that we could consider both $X \cup^{\delta} Y$ and $Y \cup^{-\delta} X$ as candidates while these are isomorphic. We define the function $\mathtt{adjacent}(R)$ to be set of all regions adjacent to $R$. Figure \ref{fig:adjacency} depicts more clearly what such a set looks like.

In order to compute complexity later, we would like to know the size of this set of adjacent regions. Fortunately it is trivial to give an upper bound for some region $R=(X,t,\vec{p})$.
\begin{align}
	|\mathtt{adjacent}(R)| = \big(\mathtt{width}(X)+1\big) \times \big(\mathtt{height}(X)+1\big) - |X| + 1
\end{align}
According to this, the smallest possible region whose image is a single element in $A$ has at most 4 adjacent regions. For the rest where complexity is concerned, we only need to know that the number of adjacent regions is on average roughly $O(|X|^{1.3})$.

\subsection{The baseline algorithm}

Now that we have defined and motivated the heuristics we use, it is time to discuss the algorithm itself. We will with an outline given by Listing \ref{alg:vouw}.

\begin{algorithm}
\caption{vouw}
\label{alg:vouw}
\begin{algorithmic}
\State $h \gets H_A^0$
\State $\bar{h} \gets \bar{H}_A^0$
\Repeat
	\State $C \gets \emptyset$ 
		\Comment Set of candidates
	\State $\forall_{c\in C} \ {usage}(c) = 0$
		\Comment Relation $C \to \mathbb{Z}$ stores occurrence per candidate
	\ForAll{$R=(X,p) \in \bar{h}$}
		\Comment Pattern $X$ with pivot $p$ in the instance set
		\ForAll{$S=(Y,q) \in \mathtt{adjacent}(R)$}
		\State $\delta \gets q-p$
		\State $c \gets (X,Y,\delta)$
			\Comment Candidate tuple
		\State $C = C \cup c$
		\State ${usage}(c) = {usage}(c) + 1$ 
		\EndFor
	\EndFor	
	\State $c_{best} = (X,Y,\delta) \in C : \forall_{c \in C} \ \mathtt{gain}(c) \leq \mathtt{gain}(c_{best})$
		\Comment Select best candidate
	\State $g = \mathtt{gain}(c_{best})$
	\If{$g > 0 $}
		\State $h = h \cup (X \cup^\delta Y)$
			\Comment Add the union pattern to the model
		\ForAll{$ R \in \bar{h} \mid R=R_{c_{best}}$}
			\ForAll{$S \in \mathtt{adjacent}(R) \mid S = S_{c_{best}}$}
				\State $\bar{h} = \bar{h} \cap R,S \cup R+S$
					\Comment Replace R and S with their sum
			\EndFor
		\EndFor
	\EndIf
\Until{$g \leq 0$}

\end{algorithmic}
\end{algorithm} 

In the algorithm described here we have omitted the variants and equivalence sets completely for the sake of simplicity. While in further sections we will gradually make it more precise, we will now briefly focus on the overall structure. Similar to the inductive definition of the instance set (see Section \ref{thesetH}), the algorithm iteratively tries to find the most promising two patterns to combine. To this end we first look at all possible candidates: tuples $(X,Y,\delta)$ of two patterns $X,Y$ and a distance $\delta$. We then greedily take `the best' candidate. This mechanism can be divided into five steps: (1) find candidates, (2) estimate candidates' value for $\mathtt{usage}()$, (3) compute candidates' gain, (4) merge patterns $X \cup^{\delta} Y$  with the highest gain and (5) replace all matching regions with their sums. 

The key to all these steps is knowing which candidate is the most promising. By estimating the $\mathtt{usage}()$ function (Definition \ref{usage}) for the union pattern $X \cup^{\delta} Y$ we already gain insight into how useful a certain combination of patterns is: patterns that occur more often tend to reveal more structure and therefore compress the underlying data better. This is only part of the truth, however. When we for example combine two zero entries in a sparse matrix, this yields a pattern with high usage but we learn nothing about the data that we did not know already. To this end we define \emph{gain}, the number of bits the two-part MDL equation $L(H)+L(D|H)$ decreases when we would pick a certain candidate.
\begin{definition} We define $\mathtt{gain}$ of candidate $c=(X,Y,\delta)$ such that:
\begin{align*}
	\mathtt{gain}(X,Y,\delta) = &L(h) + L(\bar{h}) \\
	 - &L(h \cup \{X\cup^{\delta}Y\})\\ 
	 - &L(\bar{h} \cup \{R+S | R,S \in \bar{h}, X = X_R, Y = X_S, \delta = p_S-p_R\} \\
	 &\setminus \{R,S \in \bar{h} | X = X_R, Y = X_S, \delta = p_S-p_R\}) \\	 
\end{align*}
\end{definition}
In the definition above we slightly abused notation to let $X_R$ and $p_R$ denote pattern in region $R$ and pivot of region $R$, respectively.



%Given the matrix $A$, there are two trivial models: that of $H_A^0$, the model of all singleton patterns and the completely overfitted model $H=A$. In between these extrema there is a vast space of models of varying degrees of complexity and this complexity is determined by the size of the patterns. Let us take $\mathcal{P}_{(b)}^n$ to be the set of patterns of exactly $n$ elements given $b$ different elements. We can establish the upper bound on the size of this set to be
%$$
%|\mathcal{P}_{(b)}^n| = b^n \cdot \binom{NM-1}{n-1}.
%$$
%Here $b^n$ represents the number of combinations when we pick from $b$ different values and $\binom{NM-1}{n-1}$ gives the number of spatial configurations of the patterns. Of course the actual number of possibilities greatly depends on the distribution of $A$. 



%The general objective of the VOUW algorithm is to generate a model that optimally\footnote{Note that `optimal' rather means `good' in the context of a greedy algorithm} describes some $M\times N$ matrix $A$. In the realm of VOUW, and many other MDL algorithms, this model is defined by a \emph{codetable}, which will be defined more concretely in a bit. Given this codetable, the risiduals or the \emph{data given the model} can be unambiguously derived from $A$ in linear time. We also say that $A$ is \emph{encoded} by its code table $C(A)$. This not only gives us information about $A$ (namely, the amount of `order' in its structure) but also lets us test the codetable of $A$ against some other matrix $B$ (not neccesarily $M\times N$) in linear time and vice versa. By comparing the two-part MDL equation for both $A$ and $B$ given the eachother's code table, something can be said about the similarity of $A$ and $B$: If $B$ is very similar to $A$ it will probably be encoded well by the codetable derived from $A$. There is more than that, however, because by counting which elements from the code table are used most, we can also say \emph{why} $A$ and $B$ are similar. We will describe this exact codetable in a bit, but first let us define $A$, the input matrix.

%%%

%The search space in enumerating all possible representations of $A$ becomes truly \emph{enormous} however, so this is strictly not feasible. We need to apply some heuristic at this point and could for example use one of the many encoding schemes for (sparse) matrices or use a compression algorithm like run-length encoding (RLE). This would indoubtedly produce a more meaningfull representation of $A$ but would also discard its spatial aspects. The approach used by VOUW is to disimage spatial patterns in $A$ recursively by combining previously found patterns into increasingly large patterns. The next section formalizes this concept, after which a brief example is shown to demonstrate its effectiveness.

%%%

%Ideally the relation $\circ$ would not be needed and we could simply find all the equivalent patterns by evaluating all possibilities with the MDL equation. The same argument as before holds, however, that this is infeasible. This means that $\circ$ is a heuristic and must encode some sort of domain knowledge about $A$. The elegance in this is that we have isolated \emph{all} `prior knowledge' in one operator and that this operator can be used for both encoding and decoding.


%%%

%Using the above definitions the practical implementations of the two parts of the crude MDL equation can now be defined. An encoded data set $A^{C}$ is defined as a set of regions that completely image the original data space $A$ in a non-overlapping fashion. Using this set of regions, the original data can be restored unambiguously.

%The code table $C$ is a list of all patterns $X$ that occur in $A^C$. For each pattern the code table stores the usage (the number of times it occurs in $A^{C}$) and the code length. The code length is the theoretic number of bits that would be required to store one instance of $X$. It is defined by the probability $P(X)$ that $X$ occurs in $A^{C}$ by Shannon's Entropy\cite{shannon} $-\log P(X)$. Therefore, codes that are used very often will be smaller. A result is that rarely used patterns are discouraged by MDL through the fact that they make the code table (model) more bulky.


\end{document}