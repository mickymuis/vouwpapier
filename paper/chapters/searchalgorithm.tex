\documentclass{llncs}
\input{"../preamble.tex"}
\begin{document}

\section{The Vouw Algorithm}

Pattern mining often yields vast search spaces and geometric pattern mining is no exception. We therefore use a heuristic approach, as is common in MDL-based approaches \cite{krimp,slim,classy}. We devise a greedy algorithm that exploits the inductive definition of the search space as shown by the lattice in Figure~\ref{lattice}. We start with a completely underfit model (leftmost in the lattice), where there is one instance for each matrix element. Next, in each iteration we combine two patterns, resulting in one or more pairs of instances to be merged (i.e., we move one step right in the lattice). In each step we merge the pair of patterns that improves compression most, and we repeat this until no improvement is possible.

%%% THIS IS REDUNDANT AS IT ALREADY FOLLOWS FROM THE DEFITION OF A PATTERN
%Another decision we make a priori is that we only find patterns that are \emph{contiguous}: containing only elements that are adjacent to at least one other elements in the same pattern. This decision results in a strong focus on local structure while also dramatically reducing the search space. 

\subsection{Finding candidates}

The first step is to find the `best' \textbf{candidate} pair of patterns for merging (Algorithm 1). A candidates is denoted as a tuple $(X,Y,\delta)$, where $X$ and $Y$ are patterns and $\delta$ is the relative offset of $X$ and $Y$ as they occur in the data.
Since we only need to consider pairs of patterns and offsets that actually occur in the instance matrix, we can directly enumerate candidates from the instantiation matrix and never even need to consider the original data.  

The \textbf{support} of a candidate, written $\mathrm{sup}(X,Y,\delta)$, tells how often it is found in the instance matrix. Computing support is not completely trivial, as one candidate occurs multiple times in `mirrored' configurations, such as $(X,Y,\delta)$ and $(Y,X,-\delta)$, which are equivalent but can still be found separately. Furthermore, due to the definition of a pattern, many potential candidates cannot be considered by the simple fact that their elements are not adjacent. 

%\subsubsection{Peripheries}
\smallskip \noindent \textbf{Peripheries}. For each instance $x$ we define its \emph{periphery}: the set of instances which are positioned such that their union with $x$ produces a valid pattern. This set is split into the \emph{anterior-} $\mathrm{ANT}(X)$ and \emph{posterior} $\mathrm{POST}(X)$ peripheries, containing instances that come before and after $x$ in lexicographical order, respectively. This enables us to scan the instance matrix once, in lexicographical order. For each instance $x$, we only consider the instances $\mathrm{POST}(x)$ as candidates, thereby eliminating any (mirrored) duplicates. 

%\subsubsection{Self-Overlap}
\smallskip \noindent \textbf{Self-overlap}. Self-overlap happens for candidates of the form $(X,X,\delta)$. In this case, too many or too few copies may be counted. Take for example a straight line of five instances of $X$. There are four unique pairs of two $X$'s, but only two can be merged at a time, in three different ways. Therefore, when considering candidates of the form $(X,X,\delta)$, we also compute an \emph{overlap coefficient}. This coefficient $e$ is given by $e = (2N_X+1)\delta_i + \delta_j + N_X$, which essentially transforms $\delta$ into a one-dimensional coordinate space of all possible ways that $X$ could be arranged \emph{after} and \emph{adjacent} to itself. For each instance $x_1$ a vector of bits $V(x)$ is used to remember if we have already encountered a combination $x_1,x_2$ with coefficient $e$, such that we do not count a combination $x_2,x_3$ with an equal $e$. This eliminates the problem of incorrect counting due to self-overlap.

\begin{figure*}[t]
\vspace{-\baselineskip}
\begin{minipage}[t]{.45\textwidth}
	\begin{algorithm}[H]
	\caption{FindCandidates}
	%\label{alg:cand}
	\begin{algorithmic}[1]
	\Require $I$
	\Ensure $C$
	\ForAll{$x \in I$}
		\ForAll{$y \in \mathrm{POST}(x)$}
			\State $X \gets \oslash(x), \ Y \gets \oslash(y)$
			\State $\delta \gets \mathrm{dist}(X,Y)$
			\If{$X = Y$}
				\IfContinue{$V(x)[e] = 1$}
				\State $V(y)[e] \gets 1$
			\EndIf
			\State $C \gets C \ \cup \ (X,Y,\delta)$
			\State $\mathrm{sup}(X,Y,\delta)$ += 1
		\EndFor
	\EndFor
	\end{algorithmic}
	\end{algorithm}%
\end{minipage}%\hfill
\begin{minipage}[t]{.55\textwidth}
	\begin{algorithm}[H]
	\caption{Vouw}
	\label{alg:vouw}
	\begin{algorithmic}[1]
	\Require $H,\ I$
		\State $C \ \gets$ FindCandidates(I)
		\State $(X,Y,\delta) \in C : \forall_{c \in C} \Delta L((X,Y,\delta)) \leq \Delta L(c)$
	\State $\Delta L_{best} = \Delta L((X,Y,\delta))$
	\If{$\Delta L_{best} > 0 $}
		\State $Z \gets \oslash(X\otimes(0,0) + (Y\otimes\delta))$
		\State $H \gets H \cup \{Z\}$
		\ForAll{$x_i \in I \mid \oslash(x_i)=X$}
			\ForAll{$y \in \mathrm{POST}(x_i) \mid \oslash(y) = Y$}
				\State $x_i \gets Z$,  $y \gets \cdot$
			\EndFor
		\EndFor
	\EndIf
	\State \textbf{repeat until} $\Delta L_{best} \ < \ 0$
	\end{algorithmic}%
	\vspace{-1.75pt}
	\end{algorithm}
\end{minipage}
\end{figure*}

\subsection{Gain computation}

After candidate search we have a set of candidates $C$ and their respective supports. The next step is to select the candidate that gives the best \emph{gain}: the improvement in compression by merging the candidate pair of patterns. For each candidate $c=(X,Y,\delta)$ the gain $\Delta L(A',c)$  is comprised of two parts: (1) the negative gain of adding the union pattern $Z$ to the model $H$, resulting in $H'$, and (2) the gain of replacing all instances $x,y$ with relative offset $\delta$ by $Z$ in $I$, resulting in $I'$. We use length functions $L_1, L_2$ to derive an equation for gain:
\begin{align}
\label{gain}
\begin{split}
	\Delta L(A',c) &= \Big(L_1(H') + L_2(I') \Big) - \Big(L_1(H) + L_2(I) \Big) \\
			    &= L_0(|H|) - L_0(|H|+1) - L_p(Z) + \Big(L_2(I') - L_2(I) \Big)
\end{split}
\end{align}

As we can see, the terms with $L_1$ are simplified to $- L_p(Z)$ and the model's length because $L_1$ is simply a summation of individual pattern lengths. The equation of $L_2$ requires the recomputation of the entire instance matrix' length, which is expensive considering we need to perform it for \emph{every candidate}, \emph{every iteration}. However, we can rework the function $L_{pp}$ in Equation (\ref{plugin}) by observing that we can isolate the logarithms and generalise them into
\begin{align}
	\log_G(a,b) = \log \frac{\Gamma(a+ b\epsilon)}{\Gamma(b\epsilon)} = \log \Gamma(a+ b\epsilon) - \log \Gamma(b\epsilon),
\end{align} 

\noindent which can be used to rework the second part of Equation (\ref{gain}) in such way that the gain equation can be computed in constant time complexity.

\begin{align}
\begin{split}
	L_2(I') - L_2(I) = &\log_G(U(X),1) + \log_G(U(Y),1) \\
			      &- \log_G(U(X)-U(Z),1) - \log_G(U(Y)-U(Z),1) \\
			      &- \log_G(U(Z),1) + \log_G(|I|,|H|) - \log_G(|I'|,|H'|) \\
%			    = &\log \Gamma(U(X)+\epsilon) - \log \Gamma(\epsilon) + \log \Gamma(U(Y)+\epsilon) - \log \Gamma(\epsilon) - \log \Gamma(U(X)+U(Z)+\epsilon) + \log \Gamma(\epsilon) - \log \Gamma(U(Y)+U(Z)+\epsilon) + \log \Gamma(\epsilon) - \log \Gamma(U(Z)+\epsilon) + \log \Gamma(\epsilon) + \log \Gamma(|I|+|H|\epsilon) - \log \Gamma(|H|\epsilon) - \log \Gamma(|I|+|H'|\epsilon) + \log \Gamma(|H'|\epsilon) \\
%			    = &\log \Gamma(U(X)+\epsilon) + \log \Gamma(U(Y)+\epsilon) - \log \Gamma(U(X)+U(Z)+\epsilon) - \log \Gamma(U(Y)+U(Z)+\epsilon) - \log \Gamma(U(Z)+\epsilon) + \log \Gamma(\epsilon) + \log \Gamma(|I|+|H|\epsilon) - \log \Gamma(|H|\epsilon) - \log \Gamma(|I'|+|H'|\epsilon) + \log \Gamma(|H'|\epsilon)			    
\end{split}   
\end{align}

\noindent Notice that in some cases the usages of $X$ and $Y$ are equal to that of $Z$, which means additional gain is created by removing $X$ and $Y$ from the model. 

\subsection{Mining a Set of Patterns}

%Finding candidates and computing gain for these candidates, is the first part of the algorithm. 
In the second part of the algorithm, listed in Algorithm~\ref{alg:vouw}, we select the candidate $(X,Y,\delta)$ with the largest gain and merge $X$ and $Y$ to form $Z$, as explained in Section~\ref{constructpatterns}. We linearly traverse $I$ to replace all instances $x$ and $y$ with relative offset $\delta$  by instances of $Z$. $(X,Y,\delta)$ was constructed by looking in the posterior periphery of all $x$ to find $Y$ and $\delta$, which means that $Y$ always comes after $X$ in lexicographical order. The pivot of a pattern is the first element in lexicographical order, therefore $\mathrm{pivot}(Z) = \mathrm{pivot}(X)$. This means that we can replace all matching $x$ with an instance of $Z$ and all matching $y$ with $\cdot$. %This concludes the baseline version of the algorithm. 

\subsection{Improvements}
\label{improvements}
%\subsubsection{Local Search}

We next describe two (independent) improvements on the baseline algorithm. 

\smallskip
\noindent \textbf{Local search}. %Given a matrix containing some pattern $X$, the algorithm can arrive to $X$ in different ways. Exploring these combinatorics can tell us how efficiently the algorithm arrives at $X$. By definition we know that the fundamental operation is to combine exactly two patterns on each step. Given this, the number of steps in which $X$ can be constructed lies between $\log_2|X|$ and $|X|-1$. 
%Informally we can say that the algorithm searches for `as few and as large as possible patterns', because it tries to minimize the size of the model as well as that of the instance matrix at the same time. This means that it will try to generate pattern $X$ by adding small elements to a incrementally growing pattern, resulting in a behaviour that approaches $|X|-1$ steps. This is in contrast to a hypothetical divide-and-conquer strategy that would first generate parts of $X$ that are of sizes $2, 4, 8,$ etc, resulting in $\log_2|X|$ steps.
% We call this local search \emph{flood fill} because it is similar to the image algorithm with the same name. 
To improve the efficiency of finding large patterns without sacrificing the underlying idea of the original heuristics, we add an optional local search. Observe that without local search, Vouw generates a large pattern $X$ by adding small elements to an incrementally growing pattern, resulting in a behaviour that requires up to $|X|-1$ steps. To speed this up, we can try to `predict' which elements will be added to $X$ and add them immediately. After selecting candidate $(X,Y,\delta)$ and merging $X$ and $Y$ into $Z$, for all $m$ resulting instances $z_i \in {z_0,\dots,z_{m-1}}$ we try to find pattern $W$ and offset $\delta$ such that
\begin{align}
\label{floodfill}
%\exists w \in \mathrm{POST}(z_i) \iff 
\forall_{i\in 0\dots m} \exists_w \in \mathrm{ANT}(z_i) \cup \mathrm{POST}(z_i) \ \cdot \ \oslash(w) = W \land dist(z_i, w) = \delta.
\end{align}
\noindent This yields zero or more candidates $(Z,W,\delta)$, which are then treated as any set of candidates: candidates with the highest gain are iteratively merged until no candidates with positive gain exist. This essentially means that we run the baseline algorithm only on the peripheries of all $z_i$, with the condition that the support of the candidates is equal to that of $Z$. %Therefore we only expand $Z$ during local search and we do not create new patterns. 

%\subsubsection{Reusing Candidates}

%In the baseline algorithm, candidate search is performed on each iteration and as exactly one candidate is selected. After merging the candidate, the list of candidates must be recomputed because the usages of the patterns may have changed and thus the gain function for the current set of candidates has become invalid. 

%Candidate search is by far the most expensive operation of the algorithm and is it performed on each iteration.
\smallskip \noindent \textbf{Reusing candidates}.  We can improve performance by reusing the candidate set and slightly changing the search heuristic of the algorithm. The \textbf{Best-N} heuristic selects multiple candidates on each iteration, as opposed to the baseline \textbf{Best-1} heuristic that only selects a single candidate with the highest gain. Best-N selects candidates in descending order of gain until no candidates with positive gain are left. Furthermore we only consider candidates that are all \emph{disjoint}, because when we merge candidate $(X,Y,\delta)$, remaining candidates with $X$ and/or $Y$ have unknown support and therefore unknown gain.% On matrices with multiple unrelated patterns, or matrices with large symbol sets, this heuristic works greatly improves performance.

%By doing a breadth-first-search in the periphery of all newly created instances after the last step of the algorithm. 



%Given an input matrix, the algorithm is initialized with: (a) probability mass function of the values in the matrix, (b) singleton pattern for each unique element in the matrix and (c) the under-fit instantiation matrix containing only singletons.  The baseline algorithm is composed out of four stages that are repeated every iteration:
%\begin{enumerate}
%\item Candidate search
%\item Gain computation
%\item Candidate selection
%\item Merging of patterns and their instances
%\end{enumerate}

%\subsection{Extensions and Improvements}
%\subsubsection{Pre-processing}
%\subsubsection{Reducing Self-Overlap}
%\subsubsection{Alternative Heuristics}
%\subsubsection{Local Search}
%\subsubsection{Noise-Tolerance}


\end{document}
