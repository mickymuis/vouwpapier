\documentclass{llncs}
\input{"../preamble.tex"}
\begin{document}

\section{Geometric Pattern Mining using MDL}

%\subsection{On Patterns and Matrices}

%In this subsection we will introduce a method for describing and decomposing tabular or matrix-like data. It is this concise notation that allows us to reason about what one can and cannot expect to find from a set of data and what this data could look like. In later subsections we will use these tools to gradually connect to the MDL principle and finally to a practical search algorithm.

We define geometric pattern mining on bounded, discrete and two-dimensional raster-based data. We represent this data as an $M\times N$ matrix $A$ whose rows and columns are finite and in a fixed ordering (i.e., reordering rows and columns semantically alters the matrix). Each element $a_{i,j} \in S$, where row $i \in [0;N)$, column $j \in [0;M)$, and $S$ is a finite set of symbols, i.e., the alphabet of $A$. %We will denote $A$ as a matrix because of the convenience of the linear algebra notation\footnote{In any practical application however, $A$ will not represent a system of linear equations but rather hold some form of sampled data that can be placed on a grid.}

\begin{figure}[b]
\input{"figures/decomposition_example_small.tex"}
\label{example1}
\end{figure}

According to the MDL principle, the shortest (optimal) description of $A$ reveals all structure of $A$ in the most succinct way possible. This optimal description is only optimal if we can unambiguously reconstruct $A$ from it and nothing more---the compression is both minimal and lossless. Figure~\ref{example1} illustrates how an example matrix could be succinctly described using patterns: matrix $A$ is decomposed into patterns $X$ and $Y$. A set of such patterns constitutes the \textbf{model} for a matrix $A$, denoted $H_A$ (or $H$ for short when $A$ is clear from the context). In order to reconstruct $A$ from this model, we also need a mapping from the $H_A$ back to $A$. This mapping represents what (two-part) MDL calls \textbf{the data given the model $H_A$}. In this context we can think of this as a set of all instructions required to rebuild $A$ from $H_A$, which we call the \textbf{instantiation} of $H_A$ and is denoted by ${I}$ in the example. These concepts allow us to express matrix $A$ as a decomposition into sets of local and global spatial information, which we will next describe in more detail.

% Notice how $I_A$ essentially tells us where in $A$ each pattern from $H_A$ was originally located.

%We call a specific set of patterns $H\in \mathcal{H}_A$ a \textbf{point hypothesis} and analogously a specific instantiation is denoted ${I} \in \mathcal{I}_A$. Formally, a complete description $A'$ of $A$ is a combination of some $H$ and ${I}$ such that ${I}$ is a 1-to-1 mapping from $H_A$ to $A$. In the next subsections we will formalize these concepts by starting bottom-up with patterns and instances up to models, instantiation matrices and their link with MDL. After this we will discuss how we propose to find a good description of $A$ by means of the VOUW algorithm.


\subsection{Patterns and Instances}
\noindent $\triangleright$ \emph{We define a \textbf{pattern} as an $M_X\times N_X$ submatrix $X$ of the original matrix $A$. Elements of this submatrix may be $\cdot$, the empty element, which gives us the ability to cut-out any irregular-shaped part of $A$. We additionally require the elements of $X$ to be adjacent (horizontal, vertical or diagonal) to at least one non-empty element and that no rows and columns are empty.}
%While this limits the amount of possible patterns somewhat, it will later on also reduce the computational effort dramatically.
%We will now define a pattern to be the smallest submatrix to completely contain all elements of $X$.

%\begin{definition}
%A \textbf{pattern} $X$ is a $M_X\times N_X$ submatrix of $A$ where any non-empty element may %optionally be replaced by $\cdot$ such that:
%\begin{itemize}
%\item Any non-empty element in $X$ is adjacent (horizontal, vertical or diagonal) to at least one other non-%empty element.
%\item The first and last rows and columns contain at least one non-empty element.
%\end{itemize}
%\end{definition}
\smallskip

From this definition, the dimensions $M_X\times N_X$ give the smallest rectangle around $X$ (the \emph{bounding box)}. We also define the cardinality $|X|$ of $X$ as the number of non-empty elements. We call a pattern $X$ with $|X|=1$ a \textbf{singleton pattern}, i.e., a pattern containing exactly one element of $A$. %\footnote{We will often slightly abuse notation by using single-index elements or using set notation for any matrix. For instance, when we write $x_i \in X$, we mean that $x_i$ is the $i$-th non-empty element in $X$ with $1\leq i \leq |X|$. By convention we will always use row-major ordering in these cases.}

Each pattern contains a special \textbf{pivot} element: %
%\begin{definition}
$pivot(X)$ is the first non-empty element of $X$. % in the first non-empty column of the first (non-empty) row of $X$.
%\end{definition}
\noindent
A pivot can be thought of as a fixed point in $X$ which we can use to position its elements in relation to $A$. This translation, or \textbf{offset}, is a tuple ${q}=(i,j)$ that is on the same domain as an index in $A$. We realise this translation by placing all elements of $X$ in an empty $M\times X$ size matrix such that the pivot element is at $(i,j)$. We formalise this in the \textbf{instantiation operator} $\otimes$:

\smallskip
\noindent $\triangleright$
%\begin{definition}
%In the context of an $M\times N$ matrix $A$,
\emph{We define the \textbf{instance} $X \otimes {(i,j)}$ as the $M\times N$ matrix containing all elements of $X$ such that $\mathrm{pivot}(X)$ is at index $(i,j)$ and the distances between all elements are preserved. The resulting matrix contains no additional non-empty elements. } %
%\end{definition}
\smallskip

%So according to this definition $\otimes$ adds `padding' around the elements of a pattern to align its pivot to a certain offset $(i,j)$. 
Since this does not yield valid results for arbitrary offsets $(i,j)$, we enforce two constraints: (1) an instance must be \textbf{well-defined}: placing $\mathrm{pivot}(X)$ at index $(i,j)$ must result in an instance that contains all elements of $X$, and (2) elements of instances cannot \emph{overlap}, i.e., each element of $A$ can be described only once. %This allows for a description that is both unambiguous and minimal.

\smallskip
%\begin{definition}
\noindent $\triangleright$
\emph{Two pattern instances $X \otimes {q}$ and $Y \otimes {r}$, with ${q} \neq {r}$ are \textbf{non-overlapping} if $|(X \otimes {q}) + (Y \otimes {r})| = |X|+|Y|$.}
%\end{definition}
\smallskip

From here on we will use the same letter in lower case to denote an arbitrary instance of a pattern, e.g., $x = X \otimes {q}$ when the exact value of ${q}$ is unimportant. Since instances are simply patterns projected onto an $M\times N$ matrix, we can reverse $\otimes$ by removing all completely empty rows and columns:

\smallskip
\noindent $\triangleright$
%\begin{definition}
\emph{Let $X \otimes {q}$ be an instance of $X$, then by definition we say that $\oslash(X \otimes {q}) = X$.}
%\end{definition}
\smallskip

We briefly introduced the instantiation $I$ as a set of `instructions' of where instances of each pattern should be positioned in order to obtain $A$. As Figure \ref{example1} suggests, this mapping has the shape of an $M\times N$ matrix.

\smallskip
%\begin{definition}
\noindent $\triangleright$
\emph{Given a set of patterns $H$, the \textbf{instantiation (matrix)} ${I}$ is an $M\times N$ matrix such that ${I}_{i,j} \in H \cup \{\cdot\}$ for all $(i,j)$, where $\cdot$ denotes the empty element. For all non-empty ${I}_{i,j}$ it holds that ${I}_{i,j} \otimes (i,j)$ is a non-overlapping instance of ${I}_{i,j}$ in $A$.}
%\end{definition}
%\smallskip

%The above definition creates the interesting proposition that the offset to each instance is unique. We provide a sketch proof in Appendix \ref{appendix_a} in \cite{archive}.
%Given that a pattern's pivot is placed exactly at one offset and that instances must be non-overlapping, makes this indeed believable. Later when we inductively define the set $\mathcal{I}$ of all instantiation matrices, this will be shown to be true more formally. 

%%
%% Removing this for now...
\begin{comment}
\subsection{Geometries}

In the previous subsections we silently omitted an important constraint in instantiating patterns. An instance has only meaning in the context of matrix $A$ if their respective elements match. In order words, if an instance $X \otimes (i,j)$ has all its non-empty elements identical to the corresponding indices in $A$, this means that pattern $X$ matches $A$ in $(i,j)$. In this case the match is exact, formally
\begin{definition}
The instance ${x}=X \otimes {q}$ is an \textbf{exact match on $A$} if for all non-empty ${x}_{i,j} \in {x}$ it holds that ${x}_{i,j} = a_{i,j}$.
\end{definition}

While this form of straightforward matching can be desirable in some cases, it may lead to a bloated model in others. Take the example from Figure \ref{example3} that lists matrix $A$ and four patterns. Pattern $W$ is obviously an exact match to five of the six elements of $A$. Pattern $X$, while not an exact match, is also a good candidate to describe $A$ although it is multiplied by a constant factor of two. Pattern $Y$ is a near-exact match: only one element is off - this could be due to noise for example. Pattern $Z$ is no match for the values in $A$, but notice that it is identical in structure to the other patterns: it places as many values at the same indices. 

\begin{figure}
\input{"figures/identical_geometries.tex"}
\label{example3}
\end{figure}

The example above suggests that we could do one more step of decomposition: just like we decomposed the original matrix into global structure (instantiation) and local structure (pattern), we decompose patterns into structure and magnitude. 

\begin{definition}
The \textbf{geometry} $G(X)$ of pattern $X$ is an equally-sized Boolean matrix such that $G(X)_{i,j}=1$ whenever $X_{i,j}$ is non-empty.\\
The \textbf{magnitude} $M(X)$ of pattern $X$ is the string of values obtained by taking each non-empty element of $X$ in row-major order.
\end{definition}

In the example we can see that $G(X)= \tiny\begin{bmatrix}1 & 1 \\[-.2em]\cdot & 1 \\[-.2em]1 & 1\end{bmatrix}$ and that $M(X)=1\ 2\ 1\ 1\ 2$. In fact, because all four patterns have the same geometry, we say that they are \textbf{isomorphic}. As such we write $X \cong Y$ iff $G(X) = G(Y)$. 

To understand the importance of separating the structure and magnitude of patterns, let us briefly expand the example above. Suppose we have a large matrix $B$ that consists of $n$ clusters of matrices $W$ and $X$ from Figure \ref{example3}. To determine the optimal description of $B$, we might want to look at the prevalence of each submatrix. Say it contains $\frac{n}{2}$ $W$'s and $\frac{n}{2}$ $X$'s. In this case it makes sense to include both patterns in the description. However, we could also exploit the fact that the structure of both patterns is equivalent and make the description more concise by only storing $G(W)$ and then $M(W)$ and $M(X)$ separately. Now imagine that $B$ only contains one $W$ and $n-1$ $X$'s. In this case the one $W$ might be an anomaly that we would like to detect. However, it could also be due to noise in the data in which case we would like to describe $B$ just using $n$ $X$'s. It is impossible to make this distinction beforehand. 

One possibility for solving this problem is to let the MDL equation decide whether the stray $W$ is an anomaly or not. Recall that according to the MDL principle the most succinct description is the best. Therefore if the amount of `effort' required to transform $X$ into $W$ is small, we should probably encode that one $W$ using $X$. In that case it is considered noise, while it is probably an anomaly if doing so would yield a larger description.
\end{comment}
%%%
%%%
\subsection{The Problem and its Solution Space}

%\subsubsection{Constructing Patterns}
\label{constructpatterns}
%Patterns can be joined to create increasingly large patterns and, as we will later see, this concept forms the basis of the search algorithm. However, two patterns could be joined in any different number of ways.

Larger patterns can be naturally constructed by joining (or merging) smaller patterns in a bottom-up fashion. %We can define the exact way two patterns should be joined by enumerating the distance of their respective pivots. 
To limit the considered patterns to those relevant to $A$, instances can be used as an intermediate step. As Figure \ref{example2} demonstrates, we can use a simple element-wise matrix addition to sum two instances and use $\oslash$ to obtain a joined pattern. Here we start by instantiating $X$ and $Y$ with offsets $(1,0)$ and $(1,1)$, respectively. We add the resulting ${x}$ and ${y}$ to obtain $\oslash{z}$, the union of $X$ and $Y$ with relative offset $(1,1)-(1,0)=(0,1)$. %We formally describe this mechanism in Theorem \ref{fundamental}. As we will see later, this will become the fundamental operation of our algorithm.

\begin{figure}[t]
\centering
\input{"figures/pattern_construction.tex"}
\label{example2}
\end{figure}

%\begin{theorem}\label{fundamental}
%Given two non-overlapping instances ${x}=X\otimes {q}$ and ${y}=Y\otimes {r}$, the sum of the matrices ${x} + {y}$ is another instance. %We observe that pattern $Z=\oslash({x} + {y})$ such that ${x} + {y} = Z\otimes {q}$.
%\end{theorem}

%Notice that this sum has the limitation that two instances can only be summed if they do not overlap. While this is a serious limitation, we will show in the next subsection that it is not of any practical relevance.

\smallskip
\noindent \textbf{The Sets $\mathcal{H}_A$ and $\mathcal{I}_A$}.\label{thesetH}
%In the previous subsections we have given a means to describe a matrix $A$ in a different way, namely by means of patterns and instances. If we succeed in describing $A$, using our notation, in a more concise way than just $A$ itself, we have learned something about the local and global structure of $A$ and perhaps even about anomalies or noisy values. In this context, we see a clear relation the MDL principle and learning. 
%In order to find a short(er) description, we will first have to define our search space and the way solutions are to be constructed. 
We define the \textbf{model class} $\mathcal{H}$ as the set of all possible models for all possible inputs. Without any prior knowledge, this would be the search space. To simplify the search, however, we only consider the more bounded subset $\mathcal{H}_A$ of all possible models for $A$, and $\mathcal{I}_A$, the set of all possible instantiations for these models. To this end we first define $H_A^0$ to be the model with only singleton patterns, i.e., $H_A^0=S$, and denote its corresponding instantiation matrix by ${I}_A^0$. Given that each element of ${I}_A^0$ must correspond to exactly one element of $A$ in $H_A^0$, we see that each ${I}_{i,j} = a_{i,j}$ and so we have ${I}_A^0 = A$. 

Using $H_A^0$ and ${I}_A^0$ as base cases we can now inductively define $\mathcal{I}_A$: % of all instantiations of all models over $A$:\\
\vspace{-0.2\baselineskip}
\begin{description}[labelwidth=\widthof{\bfseries By induction}]
\item[Base case] ${I}_A^0 \in \mathcal{I}_A$
\item[By induction] If ${I}$ is in $\mathcal{I}_A$ then take any pair ${I}_{i,j},{I}_{k,l} \in {I}$ such that $(i,j)\leq(k,l)$ in lexicographical order. Then the set ${I}'$ is also in $\mathcal{I}_A$, providing ${I}'$ equals ${I}$ except:
\vspace{-1.5\baselineskip}
\begin{align*}
{I}_{i,j}' &:= \oslash \big( {I}_{i,j} \otimes (i,j) + {I}_{k,l} \otimes (k,l) \big) \\
{I}_{k,l}' &:= \cdot %\\
%{I}_{m,n}' &:= I_{m,n} \ \forall (m,n) \neq (i,j) \ \land \ (m,n) \neq (k,l)
\end{align*}
\end{description}
\vspace{-0.2\baselineskip}

\noindent This shows we can add any two instances together, in any order, as they are by definition always non-overlapping and thus valid in $A$, and hereby obtain another element of $\mathcal{I}_A$. Eventually this results in just one big instance that is equal to $A$. Note that when we take two elements ${I}_{i,j},{I}_{k,l} \in {I}$ we force $(i,j)\leq(k,l)$, not only to eliminate different routes to the same instance matrix, but also so that the pivot of the new pattern coincides with ${I}_{i,j}$. We can then leave ${I}_{k,l}$ empty.

The construction of $\mathcal{I}_A$ also implicitly defines $\mathcal{H}_A$. While this may seem odd---defining models for instantiations instead of the other way around---note that there is no unambiguous way to find one instantiation for a given model. Instead we find the following definition by applying the inductive construction:
% \footnote{Notice that this is a problem for any real-world implementation. We will describe a heuristic to derive instantiation matrices from model and data in the next section.}.
%
%\noindent $\triangleright$
%\begin{definition}
%\emph{The set $\mathcal{H}_A$ of all models over $A$ is given by:}
\begin{align}
\mathcal{H}_A=\big\{\{\oslash({x}) \ | \ {x} \in {I} \} \ \big | \ {I} \in \mathcal{I}_A \big\}.
\end{align}
%\end{definition}

\noindent So for any instantiation ${I}\in \mathcal{I}_A$ there is a corresponding set in $\mathcal{H}_A$ of all patterns that occur in ${I}$. This results in an interesting symbiosis between model and instantiation: increasing the complexity of one decreases that of the other. This construction gives a tightly connected lattice as shown in Figure \ref{lattice}. 
%\begin{theorem}
%Given any instantiation ${I}\in \mathcal{I}_A$ and its corresponding model in $%\mathcal{H}_A$, the matrix $A$ can be reconstructed unambiguously.
%\end{theorem}

\begin{figure}[t]
\input{"figures/lattice2.tex"}
\label{lattice}
\end{figure}

%\subsubsection{The Optimal Model}

%The final step in the process of describing $A$ is to select the `best' or `most optimal' model from the set of $\mathcal{H}_A$. While intuitively we can understand that not every model we pick is equally fitting in terms of how well it describes $A$, this concepts needs to be formalized before we can even begin to search $\mathcal{H}_A$. To this end we make the connection with the MDL principle that says, informally, that the most concise encoding of the data gives us the best description. Since we are using two-part MDL, it is very convenient that we have already split the problem into two parts:  a model $H_A$ and an instantiation ${I}_A$. Together they form $A'$ and according to MDL, their sum must be minimized. The two-part MDL equation is given by:
%$$
%L_1(H_A) + L_2(A|H_A)
%$$
%Here the functions $L_1, L_2$ are two independent length functions that make up the coding scheme for two-part MDL. This minimization is often thought of as compression, although we will not actually write any encoded data. This leaves us with the task of finding an encoding scheme that encodes both model and instantiations lossless and without redundancy.

%Say we have some set of code words $\{C_0,C_1,\dots,C_n\}$. These symbols could be for example be a code word for each pattern $X_0, X_1,\dots,X_n$ in a model. We now want to find optimal lengths $l_0,l_1,\dots,l_n$ to assign to each code word using the Kraft-inequality such that holds that $\displaystyle\sum^N_{i=0}r^{-l_i} \leq 1$. In this case we say that $r=2$ (symbols 0 and 1), so we can measure code lengths in bits. 
%The Kraft-inequality gives us a bijection between code lengths and probability distributions. This is one of the main ideas of Shannon's entropy, which plays also an important role in the MDL principle. In our example we can write a probability distribution $P(X)$ for $X \in H_A$, as the probability of pattern $X$ occurring in our instance set. Given these probabilities we can use $l_i=-\log(P(X_i))$ to compute the exact number of bits a pattern should optimally be encoded with\footnote{Notice how common code words are shorter then rarely used code words. According to the Kraft inequality this gives us an optimal code. The number of bits we compute are real numbers and not integers. While this does certainly not result in a practical encoding, for the purpose of model selection we do not actually need to encode the data as we are only interested in its hypothetical length.}.

\subsection{Encoding Models and Instances}

From all models in $\mathcal{H}_A$ we want to select the model that describes $A$ best. Two-part MDL \cite{grunwaldmdl} tells us to choose that model that minimises the sum of  $L_1(H_A) + L_2(A|H_A)$, where $L_1$ and $L_2$ are two functions that give the length of the model and the length of `the data given the model', respectively. In this context, the data given the model is given by $I_A$, which represents the accidental information needed to reconstruct the data $A$ from $H_A$.

In order to compute their lengths, we need to decide how to encode $H_A$ and $I$. As this encoding is of great influence on the outcome, we should adhere to the conditions that follow from MDL theory: (1) the model and data must be encoded losslessly; and (2) the encoding should be as concise as possible, i.e., it should be optimal. Note that for the purpose of model selection we only need the length functions; we do not need to actually encode the patterns or data.

 
%To be able to compute the length of a given code word we must know the probability of that word occurring in our data. This information must also be available to the hypothetical decoder as otherwise the encoding is not lossless. Sometimes this is not practical as we do not know the probability distribution beforehand. For example, given an arbitrary encoded $A'$, we do not know the probability that each pattern occurs in the instantiation matrix. We could also encode this information and pass it to the decoder separately, but this is generally a bad idea. It cannot be stressed enough that the leanest encoding gives us the most information about the true compression ratio we achieve, while bookkeeping and meta-information only incur an undue bias. 

%The fictional encoder sequentially sends each symbol in the datastream to the `decoder' using a code word. Information theory tells us that we optimal length of a code word is given by $-\log(p)$\footnote{We calculate lengths in bits and therefore all logarithms in this paper have base 2.}, where $p$ is the exact probability that the code word occurs in the output. We therefore need not compute the actual code words, just their probabilities. For this to work, both the encoder and hypothetical decoder must know either the exact probability distribution or agree upon an approximation beforehand. Such approximation is often called a \textbf{prior} and is used to fix `prior knowledge' that does not have to be encoded explicitly. 

% A good example of a prior that we will be using is the universal code for integers \cite{integerprior}. The corresponding length function $L_{\mathbb{N}}(n)$ gives the number of bits required to encode an arbitrary $n$ and is defined as $L_{\mathbb{N}}(n) = \log*(n) + \log(c_0)$ with $\log*(n) = \log(n) + \log \log(n) + \cdots$ and $c_0=2.865064$ to satisfy the Kraft-inequality. This code is obviously not uniform and assigns a longer code to a larger $n$. We will use this code to encode arbitrary integers.

%Lastly, in addition to $L_{\mathbb{N}}$ and $L_{pp}$ we also define the length of the uniform distribution $L_0(n)=log(n)$. That is, when $n$ items have equal probability they all receive a code of equal length $log(n)$.

\smallskip
\noindent \textbf{Code length functions}.
Although the patterns in $H$ and instantiation matrix $I$ are all matrices, they have different characteristics and thus require different encodings. For example, the size of $I$ is constant and can be ignored, while the sizes of the patterns vary and should be encoded. Hence we construct different length functions\footnote{We calculate code lengths in bits and therefore all logarithms have base 2.} for the different components of $H$ and $I$, as listed in Table \ref{tablelength}. 

When encoding $I$, we observe that it contains each pattern $X\in H$ multiple times, given by the \textbf{usage} of $X$. Using the \textbf{prequential plug-in code} \cite{grunwaldmdl} to encode $I$ enables us to omit encoding these usages separately, which would create unwanted bias. The prequential plug-in code gives us the following length function for $I$. We use $\epsilon = 0.5$ and elaborate on its derivation in the Appendix\footnote{The appendix is available on \url{https://arxiv.org/abs/1911.09587}.}.
\begin{align}
\label{plugin}
	L_{pp}({I}\mid P_{plugin}) &= -\sum^{|H|}_{X_i \in h} \left[ \log \frac{\Gamma(\mathrm{usage}(X_i)+\epsilon)}{\Gamma(\epsilon)}\right] + \log \frac{\Gamma(|{I}| + \epsilon|H|)}{\Gamma(\epsilon|H|)}
\end{align}

\begin{table}[t]
\centering
\vspace{-\baselineskip}
\caption{Code length definitions. Each row specifies the code length given by the first column as the sum of the remaining terms.}
\label{tablelength}
\vspace{3pt}
\begin{tabular*}{\textwidth}{l @{\extracolsep{\fill}}lclcl}
\toprule
 & Matrix  &  Bounds & \# Elements & Positions & Symbols \\ 
\midrule
%$L_p(X)$ & Pattern & $L_0(MN)$ & \multicolumn{2}{c}{$L_{\mathbb{N}}(\binom{M_XN_X}{|X|})$} & $L_0(|S|)$\\
%$L_1(H)$ & Model & \emph{N/A} & $L_N(|H|)$ & \emph{N/A} & $L_p(X \in H)$ \\
%$L_2({I})$ & Inst. mat.& \emph{constant} & $L_0(MN)$ & \emph{implicit} & $L_{pp}({I})$\\
%$L_3(E)$ & Error mat. & \emph{constant} & $L_0(MN)$ & $L_0(MN)$ & $L_0(|S|)$\\
$L_p(X)$ & Pattern & $\log(MN)$ & \multicolumn{2}{c}{$L_{N}\binom{M_XN_X}{|X|}$} & $|X| \log(|S|)$\\
$L_1(H)$ & Model & \emph{N/A} & $L_N(|H|)$ & \emph{N/A} & $\sum_{X \in H} L_p(X)$ \\
$L_2({I})$ & Instantiation & \emph{constant} & $\log(MN)$ & \emph{implicit} & $L_{pp}({I})$\\
\bottomrule
\end{tabular*}
\end{table}

Each length function has four terms. First we encode the total size of the matrix. Since we assume $MN$ to be known/constant, we can use this constant to define the uniform distribution $\frac{1}{MN}$, so that $\log{MN}$ encodes an arbitrary index of $A$. Next we encode the number of elements that are non-empty. For patterns this value is encoded together with the third term, namely the positions of the non-empty elements. We use the previously encoded $M_XN_X$ in the binominal function to enumerate the ways we can place the $|X|$ elements onto a grid of $M_XN_X$. This gives us both \emph{how many} non-empties there are as well as \emph{where} they are. Finally the fourth term is the length of the actual symbols that encode the elements of matrix. In case we encode single elements of $A$, we assume that each unique value in $A$ occurs with equal probability; without other prior knowledge, using the uniform distribution has minimax regret and is therefore optimal. For the instance matrix, which encodes symbols to patterns, the prequential code is used as demonstrated before. Note that $L_N$ is the universal prior for the integers \cite{univinteger}, which can be used for arbitrary integers and penalises larger integers.

%\subsubsection{Geometries and the error matrix}

%TODO

\end{document}
