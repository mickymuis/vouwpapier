\documentclass{llncs}
\input{"../preamble.tex"}
\begin{document}

\section{Appendix}
\label{appendix_a}
\subsection{Prequential Plugin-Code}

To encode the instance matrix we use the \textbf{prequential plug-in code} \cite{grunwaldmdl}. The prequential plug-in code is defined for sequences of one item at a time and updates the probability of each item as it is encoded, such that the probability need not be known in advance. It has the favorable property of being asymptotically equal to the optimal code for large sequences. Say we want to encode all elements ${I}_i \in {I}$, we define:
%\begin{definition}
\begin{align}
P_{plugin}( y_i = {I}_i \mid y^{i-1} ) = \frac{|\{y \in y^{i-1} \mid y = {I}_i\}| + \epsilon }{\sum_{X \in H}|\{y \in y^{i-1} \mid y = X\}| + \epsilon}
\end{align}
%\end{definition}
Here $y_i$ is the i-th element to be encoded and $y^{i-1}$ is the sequence of elements encoded so far. We initialize the base case (no element has been sent yet) with a pseudocount $\epsilon$, which gives $P_{plugin}( y_1 = {I} \mid y^{0} ) = \frac{\epsilon}{\epsilon|H|}$. We pick $\epsilon=0.5$ as it is used generally with good results.

Let us adapt this principle to the problem of encoding patterns. The first step here is to determine the probability that each unique element (instance of a pattern) in ${I}$ occurs. 

%\begin{definition}
\label{usage}
\smallskip \noindent $\triangleright$
\emph{Given a set of instances ${I}$, we define $\mathrm{usage}(X) = |\{ {I}_i \in {I} \mid {I}_i = X\}|.$}
\smallskip
%\end{definition}

From this definition we see that the \textbf{usage} of a pattern is a sum of how often it occurs as an instance. We can use this function to simplify things a little by realizing that we actually know the precise number of instances per pattern on the side of the decoder, but not as the decoder. This information can be used to slightly rephrase Equation \ref{plugin} to be able to encode items in arbitrary order. This produces the length function of the instance matrix ${I}$ as follows\footnote{Here we use the fact that we can interchange sums of logarithms with logarithms of products and that those terms can be moved around freely. Moreover we convert the real-valued product sequences to the Gamma function $\Gamma$, which is the factorial function extended to real and complex numbers such that $\Gamma(n) = (n-1)!$.}:
\begin{align}
\begin{split}
	L_{pp}({I}\mid P_{plugin}) &= \sum^{|{I}|}_{i=1} -\log \frac{|\{y \in y^{i-1} \mid y = {I}_i\}| + \epsilon }{\sum_{X \in H}|\{y \in y^{i-1} \mid y = X\}| + \epsilon}\\
	&= \sum^{|H|}_{X_i \in h} -\log \prod^{\mathrm{usage}(X_i)-1}_{j=0} \frac{j+\epsilon}{\sum^{i-1}_{k=1} U(X_k)+j+\epsilon|H|} \\
	&= -\log \frac{\prod^{X_i\in H} \prod^{\mathrm{usage}(X_i)}_{j=0} j + \epsilon}{\prod^{|{I}|-1}_{j=0} j + \epsilon|H|} \\
	&= -\sum^{|H|}_{X_i \in h} \left[ \log \frac{\Gamma(\mathrm{usage}(X_i)+\epsilon)}{\Gamma(\epsilon)}\right] + \log \frac{\Gamma(|{I}| + \epsilon|H|)}{\Gamma(\epsilon|H|)}
\end{split}
\end{align}

%Lastly, in addition to $L_{\mathbb{N}}$ and $L_{pp}$ we also define the length of the uniform distribution $L_0(n)=log(n)$. That is, when $n$ items have equal probability they all receive a code of equal length $log(n)$.

\subsection{Generating Synthetic Datasets}

The synthetic dataset generator Ril is a small utility that produces random matrices and their ground-truths based on several parameters. By testing the output of Vouw on these matrices against the given ground-truth, we can assess both quality and quantity of the patterns discovered by the algorithm. One of the problems with synthetic data generation is that its parameters have a wide range that may or may not have any relation with real use-cases of the algorithm. Moreover, these parameters heavily influence the performance of the algorithm in a way that may not actually be representative, putting an undue bias on the test results. In this section we explain some of the parameters of Ril and discuss what values we have chosen and why.

\subsubsection{Parameters}

We start by listing all parameters to Ril in Table \ref{table:ril}. Apart from the dimensions, each parameter may require a bit of additional explanation. The alphabet size $|S|$ is strongly connected to the random distributions. Notice that the actual symbols in $S$ are irrelevant for this purpose; they are discrete and unique values without any semantics. During the experiments we noticed two relations between $|S|$ and the performance of the algorithm. First of all, a larger $|S|$ means a larger search space and therefore longer runtime. Especially in the first iterations of the algorithm, more candidates must be considered because more combinations exist in the input. On the other hand, a smaller $|S|$ increases the chance that parts of the noise are identical to (parts of) a pattern. This may lead to false positives (pattern detected in the noise) or may be lead to a pattern not being recognized at all. 
  
$P_{\mathrm{noise}}$ and $P_{\mathrm{signal}}$ how likely each symbol in $S$ is to be picked for the noise and signal elements respectively. These distributions can be different from each other, or the same. Statistically it is more difficult to separate signal from noise if $P_{\mathrm{noise}} = P_{\mathrm{signal}}$, which is why we used this scenario exclusively during the experiments. Furthermore we picked both distributions to be uniform, i.e. each symbol has a chance of $\frac{1}{|S|}$ to occur.

\begin{table}
%\centering
\caption{Parameters to Ril and their descriptions.}
\label{table:ril}
\begin{tabular*}{\textwidth}{lll}
\toprule
% & & \multicolumn{4}{c}{Precision/Recall} & \multicolumn{4}{c}{Average time}\\
% \cmidrule(l){3-6} \cmidrule(l){7-10} 
% Size & SNR & None & Local & Best-* & Both & None & Local & Best-* & Both \\
Parameter & Symbol & Description \\
\midrule
Dimensions & $M\times N$ & Width and height of the generated matrix \\
Alphabet & $|S|$ & Number of possible values for each matrix element \\
Pat. size & $[d;D]$ & Range of pattern sizes allowed to occur in the output \\
Pat. occurrence & $[c;C]$ & Patterns occur at least $o$ and at most $O$ times \\
Target SNR & $\mathrm{SNR}_T$ & Actual SNR will be \emph{approximately} this value \\
Distribution & $P_{\mathrm{noise}}, P_{\mathrm{signal}}$ & Random distributions for either noise and signal values \\
Branching fact. & $B$ & Allows hierarchical patterns ($B>0$) or 'flat' patterns \\
& & where no pattern is a subpattern of another ($B=0$) \\
\bottomrule
\end{tabular*}
\end{table}

\emph{This part of the Appendix is a placeholder and will be updated for the definitive archive version, before the actual publication date of the paper.}

\end{document}