\documentclass{llncs}
\input{"../preamble.tex"}
\begin{document}

\section{Appendix}
\label{appendix_a}
\subsection{Prequential Plugin-Code}

To encode the instance matrix we use the \textbf{prequential plug-in code} \cite{grunwaldmdl}. The prequential plug-in code is defined for sequences of one item at a time and updates the probability of each item as it is encoded, such that the probability need not be known in advance. It has the favorable property of being asymptotically equal to the optimal code for large sequences. Say we want to encode all elements ${I}_i \in {I}$, we define:
%\begin{definition}
\begin{align}
P_{plugin}( y_i = {I}_i \mid y^{i-1} ) = \frac{|\{y \in y^{i-1} \mid y = {I}_i\}| + \epsilon }{\sum_{X \in H}|\{y \in y^{i-1} \mid y = X\}| + \epsilon}
\end{align}
%\end{definition}
Here $y_i$ is the i-th element to be encoded and $y^{i-1}$ is the sequence of elements encoded so far. We initialize the base case (no element has been sent yet) with a pseudocount $\epsilon$, which gives $P_{plugin}( y_1 = {I} \mid y^{0} ) = \frac{\epsilon}{\epsilon|H|}$. We pick $\epsilon=0.5$ as it is used generally with good results.

Let us adapt this principle to the problem of encoding patterns. The first step here is to determine the probability that each unique element (instance of a pattern) in ${I}$ occurs. 

%\begin{definition}
\label{usage}
\smallskip \noindent $\triangleright$
\emph{Given a set of instances ${I}$, we define $\mathrm{usage}(X) = |\{ {I}_i \in {I} \mid {I}_i = X\}|.$}
\smallskip
%\end{definition}

From this definition we see that the \textbf{usage} of a pattern is a sum of how often it occurs as an instance. We can use this function to simplify things a little by realizing that we actually know the precise number of instances per pattern on the side of the decoder, but not as the decoder. This information can be used to slightly rephrase Equation \ref{plugin} to be able to encode items in arbitrary order. This produces the length function of the instance matrix ${I}$ as follows\footnote{Here we use the fact that we can interchange sums of logarithms with logarithms of products and that those terms can be moved around freely. Moreover we convert the real-valued product sequences to the Gamma function $\Gamma$, which is the factorial function extended to real and complex numbers such that $\Gamma(n) = (n-1)!$.}:
\begin{align}
\begin{split}
	L_{pp}({I}\mid P_{plugin}) &= \sum^{|{I}|}_{i=1} -\log \frac{|\{y \in y^{i-1} \mid y = {I}_i\}| + \epsilon }{\sum_{X \in H}|\{y \in y^{i-1} \mid y = X\}| + \epsilon}\\
	&= \sum^{|H|}_{X_i \in h} -\log \prod^{\mathrm{usage}(X_i)-1}_{j=0} \frac{j+\epsilon}{\sum^{i-1}_{k=1} U(X_k)+j+\epsilon|H|} \\
	&= -\log \frac{\prod^{X_i\in H} \prod^{\mathrm{usage}(X_i)}_{j=0} j + \epsilon}{\prod^{|{I}|-1}_{j=0} j + \epsilon|H|} \\
	&= -\sum^{|H|}_{X_i \in h} \left[ \log \frac{\Gamma(\mathrm{usage}(X_i)+\epsilon)}{\Gamma(\epsilon)}\right] + \log \frac{\Gamma(|{I}| + \epsilon|H|)}{\Gamma(\epsilon|H|)}
\end{split}
\end{align}

%Lastly, in addition to $L_{\mathbb{N}}$ and $L_{pp}$ we also define the length of the uniform distribution $L_0(n)=log(n)$. That is, when $n$ items have equal probability they all receive a code of equal length $log(n)$.

\subsection{Generating Synthetic Datasets}

This section discusses the synthetic dataset generator Ril, a small utility that produces random matrices and their ground-truths based on several parameters. By testing the output of Vouw on these matrices against the given ground-truth, we can assess both quality and quantity of the patterns discovered by the algorithm. One of the problems with synthetic data generation is that its parameters have a wide range that may or may not have any relation with real use-cases of the algorithm. Moreover, these parameters heavily influence the performance of the algorithm in a way that may not actually be representative, putting an undue bias on the test results. In this section we also explain some of the parameters of Ril and discuss what values we have chosen and why.


\subsubsection{Parameters}

We start by listing all parameters to Ril in Table \ref{table:ril}. Apart from the dimensions, each parameter may require a bit of additional explanation. $|S|$ determines the size of the alphabet or the range of values that each element in the matrix can take. Notice that the actual symbols in $S$ are irrelevant for this purpose; they are discrete and unique values without any semantics. During the experiments we noticed two relations between $|S|$ and the performance of the algorithm: (1) a larger $|S|$ means a larger search space and therefore longer runtime. Especially in the first iterations of the algorithm, more candidates must be considered because more combinations exist in the input. (2) a smaller $|S|$ increases the chance that parts of the noise are identical to (parts of) a pattern. This may lead to false positives (pattern detected in the noise) which could not have been detected by the algorithm (or any algorithm, for that matter).  We have chosen to use a fixed value of $|S|=256$ throughout the experiments. The reason for this is that we wanted to minimize the effect of 'ghost patterns' mentioned above, but also make it 'difficult enough' performance-wise in order to show any potential scaling issues. Moreover, $|S|=256$ mimics regular greyscale images, which we consider a possible use-case in the future. 
  
$P_{\mathrm{noise}}$ and $P_{\mathrm{signal}}$ how likely each symbol in $S$ is to be picked for the noise and signal elements respectively. These distributions can be different from each other, or the same. Statistically it is more difficult to separate signal from noise if $P_{\mathrm{noise}} = P_{\mathrm{signal}}$, which is why we used this scenario exclusively during the experiments. Furthermore we picked both distributions to be uniform, i.e. each symbol has a chance of $\frac{1}{|S|}$ to occur.

\begin{table}
%\centering
\caption{Parameters to Ril and their descriptions.}
\label{table:ril}
\begin{tabular*}{\textwidth}{lll}
\toprule
% & & \multicolumn{4}{c}{Precision/Recall} & \multicolumn{4}{c}{Average time}\\
% \cmidrule(l){3-6} \cmidrule(l){7-10} 
% Size & SNR & None & Local & Best-* & Both & None & Local & Best-* & Both \\
Parameter & Symbol & Description \\
\midrule
Dimensions & $M\times N$ & Width and height of the generated matrix \\
Alphabet & $|S|$ & Number of possible values for each matrix element \\
Pat. size & $[d;D]$ & Range of pattern sizes allowed to occur in the output \\
Pat. occurrence & $[c;C]$ & Patterns occur at least $o$ and at most $O$ times \\
Target SNR & $\mathrm{SNR}_T$ & Actual SNR will be \emph{approximately} this value \\
Distribution & $P_{\mathrm{noise}}, P_{\mathrm{signal}}$ & Random distributions for either noise and signal values \\
Branching fact. & $B$ & Allows hierarchical patterns ($B>0$) or 'flat' patterns \\
& & where no pattern is a subpattern of another ($B=0$) \\
\bottomrule
\end{tabular*}
\end{table}

The \emph{signal-to-noise ratio} (SNR) specifies the ratio of the number of elements belonging to a pattern (signal) versus the number of noise elements. This value is normalized on $[0;1]$. Due to the greedy nature of the synthesizing algorithm, the desired target SNR is approximated rather than exact. This also means that $\mathrm{SNR}=1$ is not possible, but this is hardly relevant with respect to real-world data. We have tested against a variety of SNRs to determine its influence on the algorithm's performance. It appears that low values for SNR generally produce data that is easier to process, because it contains fewer patterns that stand out more clearly against the noise. This may seems somewhat counter-intuitive because high SNRs are associated with cleaner signals. In reality this means that the algorithm is very good at ignoring this particular noise and that it is therefore almost irrelevant in the experiments we did. Further research should therefore probably experiment with different noise types and/or different metrics than SNR.

\subsubsection{Random Patterns}

The pattern generator algorithm in Ril is very simplistic. It uses \emph{random walks} in order to find empty elements to form a pattern, while taking adjacency into account (see the definition of a pattern in Section 3.1).

\subsubsection{Flat and Branching Patterns}



\end{document}